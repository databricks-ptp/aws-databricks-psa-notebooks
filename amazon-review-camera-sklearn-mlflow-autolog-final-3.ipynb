{"cells":[{"cell_type":"markdown","source":["# Build a Scikit Learn Sentiment Analysis Model Using MLFlow and Datasets Labeled by Amazon SageMaker Ground Truth"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2a2dc37-846a-4b75-8371-111723bed46a"}}},{"cell_type":"markdown","source":["## What is Amazon SageMaker Ground Truth?\n\nAmazon SageMaker Ground Truth is a fully managed data labeling service that makes it easy to build highly accurate training datasets for machine learning. Through the SageMaker Ground Truth console, you can create custom or built-in data labeling workflows in minutes. These workflows support a variety of use cases including 3D point clouds, video, images, and text. In addition, Ground Truth offers automatic data labeling which uses a machine learning model to label your data.\n\nIn this content, we will use [Amazon Customer Reviews Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html) to classify whether the review text's sentiment is positive, negative or neutral. The demo of how to create Grouth Truth labeling job should precede this notebook.\n\nAfter the labeling is done, the resulted dataset will be saved in a specified S3 bucket. This botebook will pick up the dataset from there and continue building the model using Sckit Learn and MLFlow.\n\n## Sentiment Analysis on Amazon Review Dataset\n\nFirst, we will download the dataset from S3. The dataset is in .manifest extention but it is saved in JSON format. After the labeling job completeed, SageMaker Ground Truth put the output in manifest file. The fisrt cell reads the file and load it to pyspark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72b4435e-9b18-447d-9da1-984be4ce1b4d"}}},{"cell_type":"code","source":["df=spark.read.json(\"s3://XXXX-XXX-XXX/tko/output.manifest\") #reivew-output.txt\")\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c9d77c0-d431-4de1-8e71-ebd94176070f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Then, we are selecting 2 columns, the review text (called \"source\") and label (called \"amazon-review-camera\") to be used for model training. The mapping for the sentiment is 0 for positive, 1 for negative, and 2 for neutral."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f3b920f-3e52-4406-a858-69c5d73904f8"}}},{"cell_type":"code","source":["# Extract only the review test and sentiment\n# positive=0, nagative=1, neutral=2\ndf2 =df.select(\"source\", \"amazon-review-camera-10000\") #amazon-review-camera-metadata.class-name\ndisplay(df2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2b6737d-2372-42a8-b5bb-bff7775506e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The pyspark DataFrame will be converted to panda's DataFrame because that is the format reuiqred for data tranformation as well as for the training."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56de966c-0fff-45c3-a478-4fb95a5e82d2"}}},{"cell_type":"code","source":["import pandas as pd\n\ntrain_set_pd = df2.toPandas()\n\ndisplay(train_set_pd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96296aad-a223-4d8d-96ed-05725a435483"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The next cell transforms the review text into bigram vector. We will further transform the bigram vector using TF-IDF to prepare the training dataset. Scikit Learn has TfidfTransformer class to use out of box (no need to implement on your own!). We will compare the validation results of both datasets: bigram with and without TF-IDF transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45a0aebd-d552-48f5-bf2d-593eede68b37"}}},{"cell_type":"code","source":["from os import system, listdir\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom joblib import dump, load # used for saving and loading sklearn objects\nfrom scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n\nsystem(\"mkdir 'data_preprocessors'\")\nsystem(\"mkdir 'vectorized_data'\")\n\n# Train data\n\n# Bigram Counts\nbigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\nbigram_vectorizer.fit(train_set_pd['source'].values)\ndump(bigram_vectorizer, 'data_preprocessors/bigram_vectorizer.joblib')\n\n# bigram_vectorizer = load('data_preprocessors/bigram_vectorizer.joblib')\n\nX_train_bigram = bigram_vectorizer.transform(train_set_pd['source'].values)\nsave_npz('vectorized_data/X_train_bigram.npz', X_train_bigram)\n\n# X_train_bigram = load_npz('vectorized_data/X_train_bigram.npz')\n\n\n# Bigram Tf-Idf\nbigram_tf_idf_transformer = TfidfTransformer()\nbigram_tf_idf_transformer.fit(X_train_bigram)\n\ndump(bigram_tf_idf_transformer, 'data_preprocessors/bigram_tf_idf_transformer.joblib')\n\n# bigram_tf_idf_transformer = load('data_preprocessors/bigram_tf_idf_transformer.joblib')\n\nX_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n\nsave_npz('vectorized_data/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42ba2fa0-8ff4-4a70-ae5d-5193906a1f10"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Set up MLFlow Experiment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83583cb6-1d2d-454e-9ed6-d997b57e4536"}}},{"cell_type":"markdown","source":["MLFlow experiment will log hyperparameters and metrics, and save model as an artifact. We are using mlflow.sklearn.autolog() that does all of the logging in a single line of code."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6330c54a-4354-4c4a-9675-ccaad8e0481b"}}},{"cell_type":"code","source":["import mlflow.sklearn\nfrom mlflow import spark\n\nmlflow.sklearn.autolog()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f5f4eaa-9a26-4845-990b-7da8eed9d0e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The following cell defines the function that executes training of a Linear classifiers with Stochastic Gradient Descent (SGD) learning. With SGD, the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule. \n\nIn the last 2 lines of code, the function is called by passing 2 different training datasets, **X_train_bigram** and **X_train_bigram_tf_idf** at a time. Compare the training and validation scores after the execution completes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac6e312d-8939-43dd-87e5-87f582cf963e"}}},{"cell_type":"code","source":["import mlflow\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\nimport numpy as np\nfrom mlflow.models.signature import infer_signature\n\n\ndef train_and_show_scores(X: csr_matrix, y: np.array, title: str):\n    X_train, X_valid, y_train, y_valid = train_test_split(\n        X, y, train_size=0.75, stratify=y\n    )\n\n    clf = SGDClassifier()\n    clf.fit(X_train, y_train)\n    train_score = clf.score(X_train, y_train)\n    valid_score = clf.score(X_valid, y_valid)\n    \n    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')\n    \n    return clf\n\n# Extract the label values for y data for training.  \ny_train = train_set_pd['amazon-review-camera-10000'].values\n\n\n# Call the training function for each of the transformed dataset, bigram and bigram with TF-IDF\nmodel1 = train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\nmodel2 = train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6569fc2a-a3ba-40fc-b35d-cad000839511"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# we are chooing model1 which performed better in validation \nregistered_model_name = \"camera_review_nlp_model\"\nmlflow.sklearn.log_model(model2, artifact_path='model', registered_model_name=registered_model_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c7ed2e9-3030-4977-9d8f-b0612363cf43"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["client = mlflow.tracking.MlflowClient()\n\nregistered_model_name = \"camera_review_nlp_model\"\nmodel = client.get_registered_model(registered_model_name)\nmodel_stage = 'Production'\nmodel_version = 30\n\n# we'll capture the latest version from the above cell and change the stage to Production in order to deploy as a SageMaker model endpoint\nclient.transition_model_version_stage(registered_model_name, version=model_version, stage=model_stage)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91bb0777-ce66-41f7-a152-8531d245f943"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Once the chosen model is saved as an artifact, it is time to build a container that will host the model on SageMaker. Running the following command will build and push the Docker container to ECR on your AWS account.\n\n`mlflow sagemaker build-and-push-container`\n\nGo to the ECR console and find the image URI. \n\nCall mlflow.sagemaker.deploy() to deploy the model as a SageMaker endpoint"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d7fb7c9-4057-4cae-b0e8-85c70a4375af"}}},{"cell_type":"code","source":["import mlflow.sagemaker as mfs\n\nmodel_uri = \"models:/\" + registered_model_name + \"/\" + str(model_version) \nimage_ecr_uri = \"XXXXXX.dkr.ecr.us-east-1.amazonaws.com/mypyfunc11:1.11.0\"\napp_name = \"camera-review\"\nregion = \"us-east-1\"\n\"\"\"\nDeploy the model from the run version. \nECR image is created in the previous step. Basically, you will add the model to the image and then deploy it as SageMaker endpoint\"\"\"\n#mlflow.sagemaker.push_image_to_ecr(image='mlflow-pyfunc')\n#mlflow.sagemaker.run_local(model_uri, port=5000, image='mlflow-pyfunc', flavor=None)\n\n\"\"\" app_name here is going to be the name of the SageMaker endpoint\"\"\"\n# Use DEPLOYMENT_MODE_CREATE to create a new endpoint\n# mfs.deploy(app_name=app_name, model_uri=model_uri, image_url=image_ecr_url, region_name=region, mode=DEPLOYMENT_MODE_CREATE) \n\n# Use DEPLOYMENT_MODE_REPLACE to update the existing endpoint\nmfs.deploy(app_name=app_name, model_uri=model_uri, image_url=image_ecr_uri, region_name=region, mode=mlflow.sagemaker.DEPLOYMENT_MODE_REPLACE) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60472aca-4f3b-4e7b-94ad-3f5656a27332"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The query_endpoint() function calls invoke_endpoint() API for an inference."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4e2af8a-6d69-4c96-a976-8fc3e8cbe76e"}}},{"cell_type":"code","source":["import json\nimport boto3\n\n# positive=0, nagative=1, neutral=2\ndef map_sentiment(prediction):\n  if prediction == 0 or \"0\":\n      return \"positive\"\n  elif prediction == 1 or \"1\": \n      return \"negative\"\n  elif prediction == 2 or \"2\": \n      return \"neutral\"\n    \n    \ndef query_endpoint(app_name, inference_data):\n  client = boto3.session.Session().client(\"sagemaker-runtime\", region)\n  \n  response = client.invoke_endpoint(\n      EndpointName=app_name,\n      Body=inference_data,\n      ContentType='application/json; format=pandas-split'\n  )\n  preds = response['Body'].read().decode(\"ascii\")\n  preds = json.loads(preds)\n  \n  print(\"Received prediction: {}\".format(map_sentiment(preds)))\n  #print(\"Raw response: {}\".format(response))\n\n  return preds\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f4c1dcc-d2cb-4753-a406-380250491dfc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Just for testing purposes, we are using the tranformed data to pass to the endpoint to see the result. \n\nFollowing prints the review to be sent for sentiment prediction and its expected result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c5a8269-87b8-4a52-b2a5-adefd2cef7b3"}}},{"cell_type":"code","source":["\"\"\" Display the test data and expected sentiment before sending it to the endpoint \"\"\"\nindex = 101\n\nprint(\"Expected sentiment of the test data: \" + map_sentiment(train_set_pd.loc[index,'amazon-review-camera-10000']))\nprint(\"Text of the test data: \" + str(train_set_pd.loc[index,'source']))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7e772f2-5725-4157-acdc-9cff4e827b66"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Making an inference call and we got the expected inference result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86c121d6-1b8f-4a32-8c89-9f23df553745"}}},{"cell_type":"code","source":["import io\n\n#X_train_bigram = X_train_bigram_tf_idf\ndata_df = pd.DataFrame.sparse.from_spmatrix(X_train_bigram_tf_idf[index]).to_json(orient='split')\n\n# Evaluate the input by posting it to the deployed model\n# positive=0, nagative=1, neutral=2\n\nprediction1 = query_endpoint(app_name=app_name, inference_data=data_df)\nprint(prediction1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fba59142-902c-4298-8fe7-bc2fdad02e31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Clean up the endpoint after testing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af552d25-9571-4424-a7c5-a0c214e3c02d"}}},{"cell_type":"code","source":["mfs.delete(app_name=app_name, region_name=region)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4bf8653-a201-4d67-ab64-48da4a451ace"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["reference: https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b361277e-f800-4418-ad43-ba2ce474fe91"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"amazon-review-camera-sklearn-mlflow-autolog-final","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1761468159814464}},"nbformat":4,"nbformat_minor":0}
